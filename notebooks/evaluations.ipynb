{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b8660a",
   "metadata": {},
   "source": [
    "# Evaluation and Analysis of Synthetic Survey Responses\n",
    "\n",
    "This notebook demonstrates how to evaluate synthetic survey responses generated by LLMs and analyze their performance. This is part of the **How Many Human Survey Respondents is a Large Language Model Worth? An Uncertainty Quantification Perspective** project. \n",
    "\n",
    "## Overview\n",
    "The evaluation pipeline consists of three main steps:\n",
    "\n",
    "1. **Run Evaluations** (`evaluations()`): \n",
    "   - Evaluates all models on the specified dataset\n",
    "   - Performs train-test splits to find optimal sample sizes (k_hat)\n",
    "   - Computes miscoverage rates, confidence interval half-widths, and other metrics\n",
    "   - Saves results to JSON files for later visualization\n",
    "\n",
    "2. **Generate Plots** (`plot_from_saved_evaluations()`):\n",
    "   - Creates visualizations from saved evaluation results\n",
    "   - Generates plots for kappa_hat, synthetic CI half-width, and test miscoverage rates\n",
    "   - Compares performance across different models and alpha values\n",
    "\n",
    "3. **Sharpness Analysis** (`sharpness_analysis()`):\n",
    "   - Analyzes the sharpness of confidence intervals\n",
    "   - Visualizes miscoverage rates as a function of sample size k\n",
    "   - Helps identify optimal sample sizes for each model\n",
    "\n",
    "Runtime note: `src/evaluations.py` now caches real confidence intervals and per-question prefix statistics for the synthetic answers, so repeated evaluations (e.g., during sweeps over many random splits) run significantly faster.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Alpha (α)**: Significance level for synthetic confidence intervals. A smaller alpha means higher confidence (e.g., α=0.05 → 95% confidence).\n",
    "- **Gamma (γ)**: Coverage probability for real confidence intervals (0.5 for 50% coverage).\n",
    "- **k_hat**: Optimal number of synthetic samples needed to achieve desired coverage.\n",
    "- **Miscoverage Rate**: Proportion of cases where the synthetic CI fails to cover the real population parameter.\n",
    "- **Coverage Types**:\n",
    "  - **General**: Confidence set inclusion test\n",
    "  - **Simple**: Empirical mean inclusion test\n",
    "\n",
    "## Usage Instructions\n",
    "1. **Configure parameters**: Set dataset name, models, alpha values, and other evaluation parameters in the configuration cell below.\n",
    "\n",
    "2. **Run evaluations**: Execute `evaluations()` to compute evaluation metrics. This may take some time depending on the number of models, splits, and k_max.\n",
    "\n",
    "3. **Generate plots**: Execute `plot_from_saved_evaluations()` to create visualizations from the saved results.\n",
    "\n",
    "4. **Analyze sharpness**: Execute `sharpness_analysis()` for detailed sharpness analysis of specific models.\n",
    "\n",
    "## Key Parameters\n",
    "\n",
    "- **`dataset_name`** (str): Dataset to evaluate. Options: `'EEDI'` or `'OpinionQA'`\n",
    "- **`models`** (list): List of model names to evaluate (must match synthetic answer file names)\n",
    "- **`alphas`** (list): Significance levels to evaluate (e.g., `[0.05, 0.10, 0.15, 0.20]`)\n",
    "- **`gamma`** (float): Coverage probability for real CIs (default: 0.5)\n",
    "- **`k_max`** (int): Maximum number of synthetic samples to evaluate (200 for OpinionQA, 60 for EEDI)\n",
    "- **`C`** (float): Scaling constant for synthetic CI half-width (default: 2)\n",
    "- **`train_proportion`** (float): Proportion of questions for training (default: 0.6)\n",
    "- **`num_splits`** (int): Number of train-test splits for robust statistics (default: 100)\n",
    "- **`CI_type`** (str): Type of confidence interval. Options: `'clt'`, `'hoeffding'`, `'bernstein'`\n",
    "\n",
    "## Output Files\n",
    "\n",
    "Results are saved to `data/{dataset_name}/{evaluation_results_folder_name}/`:\n",
    "- `general/reports_all.json`: Evaluation reports for general coverage type\n",
    "- `simple/reports_all.json`: Evaluation reports for simple coverage type\n",
    "- `general/sharpness_analysis_all.json`: Sharpness analysis results for general coverage type\n",
    "- `simple/sharpness_analysis_all.json`: Sharpness analysis results for simple coverage type\n",
    "- `general/{metric}.pdf`: Plots for general coverage type\n",
    "- `simple/{metric}.pdf`: Plots for simple coverage type\n",
    "\n",
    "For more details, see the function documentation in `src/evaluations.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cacf096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup: Import required modules and configure the Python path.\n",
    "\n",
    "This cell:\n",
    "1. Finds the project root directory by locating src/evaluations.py\n",
    "2. Adds the project root to sys.path so we can import from src/\n",
    "3. Imports all evaluation functions from src.evaluations\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Find project root by walking up from current directory until we find src/evaluations.py\n",
    "# This works regardless of where the notebook is run from\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "ROOT_DIR = cwd\n",
    "\n",
    "# Walk up directory tree to find the directory containing src/evaluations.py\n",
    "while not os.path.exists(os.path.join(ROOT_DIR, 'src', 'evaluations.py')):\n",
    "    parent = os.path.dirname(ROOT_DIR)\n",
    "    if parent == ROOT_DIR:  # Reached filesystem root\n",
    "        raise FileNotFoundError(f\"Could not find src/evaluations.py. Started from: {cwd}\")\n",
    "    ROOT_DIR = parent\n",
    "\n",
    "# Add project root to Python path\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "# Import from src.evaluations\n",
    "from src.evaluations import *\n",
    "\n",
    "print(f\"\\u2705 Successfully imported evaluation functions from {ROOT_DIR}/src/evaluations.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ec465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration: Set evaluation parameters.\n",
    "\n",
    "Modify these parameters to customize your evaluation:\n",
    "- Dataset and models to evaluate\n",
    "- Alpha values (significance levels) to test\n",
    "- Evaluation settings (k_max, C, train_proportion, etc.)\n",
    "\"\"\"\n",
    "# Dataset configuration\n",
    "dataset_name = 'OpinionQA'  # Options: 'EEDI' or 'OpinionQA'\n",
    "\n",
    "# Models to evaluate (must match synthetic answer file names)\n",
    "models = [\n",
    "    'claude-3.5-haiku', \n",
    "    'deepseek-v3', \n",
    "    'gpt-3.5-turbo', \n",
    "    'gpt-4o-mini', \n",
    "    'gpt-4o', \n",
    "    'gpt-5-mini', \n",
    "    'llama-3.3-70B-instruct-turbo', \n",
    "    'mistral-7B-instruct-v0.3', \n",
    "    'random'  # Random baseline for comparison\n",
    "]\n",
    "\n",
    "# Folder names\n",
    "synthetic_answer_folder_name = 'synthetic_answers'  # Folder containing synthetic answers\n",
    "evaluation_results_folder_name = 'evaluation_results'  # Folder to save evaluation results\n",
    "\n",
    "# Significance levels to evaluate (alpha values)\n",
    "# Each alpha corresponds to a confidence level: 1 - alpha\n",
    "# Example: alpha=0.05 → 95% confidence level\n",
    "alphas = [0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "# Coverage probability for real confidence intervals\n",
    "gamma = 0.5  # 50% coverage\n",
    "\n",
    "# Maximum number of synthetic samples to evaluate\n",
    "k_max = 200  # Use 200 for OpinionQA, 100 for EEDI\n",
    "\n",
    "# Scaling constant for synthetic CI half-width\n",
    "C = 2  # Higher C → wider (more conservative) confidence intervals\n",
    "\n",
    "# Proportion of questions to use for training\n",
    "train_proportion = 0.6  # 60% training, 40% testing\n",
    "\n",
    "# Minimum k value required for valid synthetic CI\n",
    "k_min = 2  # Minimum number of samples needed (must be at least 2)\n",
    "\n",
    "# Type of confidence interval to compute\n",
    "CI_type = 'bernstein'  # Options: 'clt', 'hoeffding', 'bernstein'\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 0\n",
    "\n",
    "# Number of train-test splits for robust statistics\n",
    "num_splits = 100  # More splits → more reliable but slower\n",
    "\n",
    "print(\"\\u2705 Configuration loaded successfully\")\n",
    "print(f\"   Dataset: {dataset_name}\")\n",
    "print(f\"   Models: {len(models)} models\")\n",
    "print(f\"   Alpha values: {alphas}\")\n",
    "print(f\"   k_max: {k_max}, CI_type: {CI_type}, num_splits: {num_splits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e58905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Run evaluations for all models.\n",
    "\n",
    "This function:\n",
    "- Loads real answers from the dataset and synthetic answers from all models\n",
    "- Performs train-test splits to find optimal sample sizes (k_hat)\n",
    "- Computes miscoverage rates, confidence interval half-widths, and other metrics\n",
    "- Saves results to JSON files for later visualization\n",
    "\n",
    "Execution time: Depends on number of models, splits, and k_max. \n",
    "Expect several minutes to hours for large evaluations.\n",
    "\n",
    "Results are saved to: data/{dataset_name}/{evaluation_results_folder_name}/\n",
    "\"\"\"\n",
    "\n",
    "evaluations(\n",
    "    dataset_name=dataset_name,\n",
    "    models=models,\n",
    "    synthetic_answer_folder_name=synthetic_answer_folder_name, \n",
    "    evaluation_results_folder_name=evaluation_results_folder_name,\n",
    "    alphas=alphas,\n",
    "    gamma=gamma,\n",
    "    k_max=k_max,\n",
    "    C=C,\n",
    "    train_proportion=train_proportion,\n",
    "    k_min=k_min,\n",
    "    CI_type=CI_type,\n",
    "    seed=seed,\n",
    "    num_splits=num_splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03413ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Generate plots from saved evaluation results.\n",
    "\n",
    "This function creates visualizations comparing all models:\n",
    "- kappa_hat: Optimal sample size (k_hat) as a function of alpha\n",
    "- synth_CI_width: Synthetic confidence interval half-width\n",
    "- test_miscov_rate: Test miscoverage rate\n",
    "\n",
    "Plots are generated for both 'general' and 'simple' coverage types.\n",
    "Plots are saved as PDF files in the evaluation_results folder.\n",
    "\n",
    "Note: This function reads from saved JSON files, so Step 1 must be completed first.\n",
    "\"\"\"\n",
    "plot_from_saved_evaluations(\n",
    "    dataset_name=dataset_name,\n",
    "    evaluation_results_folder_name=evaluation_results_folder_name,\n",
    "    num_splits=num_splits,\n",
    "    alphas=alphas,\n",
    "    gamma=gamma,\n",
    "    C=C,\n",
    "    types=['simple', 'general']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: Sharpness analysis\n",
    "\"\"\"\n",
    "\n",
    "sharpness_analysis(\n",
    "    dataset_name=dataset_name,\n",
    "    evaluation_results_folder_name=evaluation_results_folder_name,\n",
    "    type='simple',\n",
    "    histogram_model='gpt-4o'\n",
    ") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
